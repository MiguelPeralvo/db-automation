# mlFlow Registry QA & Prod pipeline

variables:
- group: Databricks-environment
- group: Pipeline-variables

schedules:
- cron: "00 17 * * *"
  displayName: Daily midnight build
  branches:
    include:
      - master

trigger:
  branches:
    include:
      - dev
      - master
      - staging
      - releases/*
  tags:
    include:
      - v*.*
      - prod

  paths:
    include:
      - libraries/*
      - pipeline/*
      - cicd-scripts/*
      - azure-pipelines.yml

stages:
- stage: Build
  condition: |
    or
    (
      eq(variables['Build.SourceBranch'], 'refs/heads/dev'),
      eq(variables['Build.SourceBranch'], 'refs/heads/master'),
      startsWith(variables['Build.SourceBranch'], 'refs/pull')
    )
  jobs:
  - job: Build
    pool:
      vmImage: 'ubuntu-18.04'

    steps:
    - script: env | sort
      displayName: 'Environment / Context'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.7'
      inputs:
        versionSpec: 3.7

    - script: |
        pip install pytest requests setuptools wheel pyspark==2.4.5
    #    pip install -U databricks-cli
      displayName: 'Load Python Dependencies'

    - checkout: self
      persistCredentials: true
      clean: true
      displayName: 'Checkout & Build.Reason: $(Build.Reason) & Build.SourceBranchName: $(Build.SourceBranchName)'

    - script: |
        echo "Build.SourceBranchName: $(Build.SourceBranchName)"
        if [ $(Build.SourceBranchName) != "merge" ]
        then
          git checkout $(Build.SourceBranchName)
        fi

      displayName: 'Get Latest from Branch $(Build.SourceBranchName) if not in PR'

    - script: |
        python -m pytest --junit-xml=$(Build.Repository.LocalPath)/logs/TEST-LOCAL.xml $(Build.Repository.LocalPath)/libraries/python/dbxdemo/test*.py || true
        ls logs
      displayName: 'Run Python Unit Tests for library code'

    - task: PublishTestResults@2
      inputs:
        testResultsFiles: '**/TEST-*.xml'
        failTaskOnFailedTests: true
        publishRunAttachments: true

    - script: |
        cd $(Build.Repository.LocalPath)/libraries/python/dbxdemo
        python3 setup.py sdist bdist_wheel
        ls dist/
      displayName: 'Build Python Wheel for Libs'

    - task: riserrad.azdo-databricks.azdo-databricks-configuredatabricks.configuredatabricks@0
      inputs:
        url: '$(WORKSPACE_REGION_URL)/?o=$(WORKSPACE_ORG_ID)'
        token: '$(DATABRICKS_TOKEN)'
      displayName: 'Configure Databricks CLI for AZDO'

      #- script: |
      #    dbfs ls
      #  displayName: 'Check config working'

    - script: |
        cat /home/vsts/.databrickscfg
        echo ""
        echo "-------------"
        dbfs cp --overwrite $(Build.Repository.LocalPath)/"pipeline/ML/train/train_model.py" dbfs:/FileStore/Shared/db-automation/train/train_model/train_model.py --profile AZDO
#        databricks workspace mkdirs /Shared/db-automation/train --profile AZDO
#        databricks workspace import $(Build.Repository.LocalPath)/"pipeline/ML/train/train_model.py" "/Shared/db-automation/train/train_model"  --language PYTHON -o --profile AZDO
      displayName: 'Import ML Train Notebook'

    - task: PythonScript@0
      inputs:
        scriptSource: 'filePath'
        scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/execute_script.py'
        arguments: '--shard $(WORKSPACE_REGION_URL) --token $(DATABRICKS_TOKEN) --cluster $(EXISTING_CLUSTER_ID) --dbfspath dbfs:/FileStore/Shared/db-automation/train/train_model/train_model.py --outfilepath /home/vsts/work/1/s/pipeline --params -e,db_automation_wine_model,-m,$(MODEL_NAME),-r,dbfs:/FileStore/Shared/db-automation'
      displayName: 'Train the new version of the model'

    - script: |
        cat /home/vsts/.databrickscfg
        echo ""
        echo "-------------"
        databricks workspace mkdirs /Shared/db-automation/batch_test --profile AZDO
        databricks workspace import $(Build.Repository.LocalPath)/"pipeline/ML/batch_test/deploy_test_databricks_batch_ml_model.py" "/Shared/db-automation/batch_test/deploy_test_databricks_batch_ml_model"  --language PYTHON -o --profile AZDO
      displayName: 'Import ML Batch Deploy/Test Notebook'

    - task: PythonScript@0
      inputs:
        scriptSource: 'filePath'
        scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/executenotebook.py'
        arguments: '--shard $(WORKSPACE_REGION_URL) --token $(DATABRICKS_TOKEN) --cluster $(EXISTING_CLUSTER_ID) --localpath $(Build.Repository.LocalPath)/pipeline/ML/batch_test --workspacepath /Shared/db-automation/batch_test --outfilepath /home/vsts/work/1/s/pipeline --params $(MODEL_NAME)'
      displayName: 'Deploy and test mlFlow Model from Registry to Databricks batch'

    - script: |
        echo $(response)
      displayName: 'Batch test result'

- stage: Release
  dependsOn: []
  condition: |
    or(
      startsWith(variables['Build.SourceBranch'], 'refs/heads/releases'),
      startsWith(variables['Build.SourceBranch'], 'refs/tags/v')
    )
  jobs:
  - job: Release
    pool:
      vmImage: 'ubuntu-18.04'

    steps:
    - script: env | sort
      displayName: 'Environment / Context'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.7'
      inputs:
        versionSpec: 3.7

    - script: |
        echo "Release"
      displayName: 'Release stage'

- stage: production
  displayName: Deploy release in production
  dependsOn: []
  condition: eq(variables['Build.SourceBranch'], 'refs/tags/production')
  jobs:
  - deployment: production
    displayName: Deployment of release in production
    pool:
      vmImage: 'ubuntu-18.04'
    environment: production
    strategy:
      runOnce:
        deploy:
          steps:
          - script: env | sort
            displayName: 'Environment / Context'

          - task: UsePythonVersion@0
            displayName: 'Use Python 3.7'
            inputs:
              versionSpec: 3.7

          - script: |
              echo "production"
            displayName: 'production stage'

#- script: |
#    cat /home/vsts/.databrickscfg
#    echo ""
#    echo "-------------"
#    databricks workspace mkdirs /Shared/db-automation/deploy --profile AZDO
#    databricks workspace import $(Build.Repository.LocalPath)/"pipeline/ML/deploy/deploy_azure_ml_model.py" "/Shared/db-automation/deploy/deploy_azure_ml_model"  --language PYTHON -o --profile AZDO
#  displayName: 'Import ML Deploy Notebook'
#
#- task: PythonScript@0
#  inputs:
#    scriptSource: 'filePath'
#    scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/executenotebook.py'
#    arguments: '--shard $(WORKSPACE_REGION_URL) --token $(DATABRICKS_TOKEN) --cluster $(EXISTING_CLUSTER_ID) --localpath $(Build.Repository.LocalPath)/pipeline/ML/deploy --workspacepath /Shared/db-automation/deploy --outfilepath /home/vsts/work/1/s/pipeline --params model_name=$(MODEL_NAME)'
#  displayName: 'Deploy mlFlow Model from Registry to Azure ML for Testing'
#
#- script: |
#    echo $(response)
#  displayName: 'API URL'
#
#- script: |
#    cat /home/vsts/.databrickscfg
#    echo ""
#    echo "-------------"
#    databricks workspace mkdirs /Shared/db-automation/test --profile AZDO
#    databricks workspace import $BUILD_SOURCESDIRECTORY/"pipeline/ML/test/test_api.py" "/Shared/db-automation/test/test_api"  --language PYTHON -o --profile AZDO
#  displayName: 'Import ML Test Notebook'
#
#- task: PythonScript@0
#  inputs:
#    scriptSource: 'filePath'
#    scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/executenotebook.py'
#    arguments: '--shard $(WORKSPACE_REGION_URL) --token $(DATABRICKS_TOKEN) --cluster $(EXISTING_CLUSTER_ID) --localpath $(Build.Repository.LocalPath)/pipeline/ML/test --workspacepath /Shared/db-automation/test --outfilepath /home/vsts/work/1/s/pipeline --params model_name=$(MODEL_NAME),scoring_uri=$(response)'
#  displayName: 'Test mlFlow Model from Registry against REST API'

#- task: PythonScript@0
#  inputs:
#    scriptSource: 'filePath'
#    scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/mlflow.py'
#    arguments: ''
#  displayName: 'Promote mlFlow Registry model to Production'
#
#- script: |
#    echo $(response)
#  displayName: 'Model Production Version'
#
#- task: PythonScript@0
#  inputs:
#    scriptSource: 'filePath'
#    scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/executenotebook.py'
#    arguments: '--shard $(WORKSPACE_REGION_URL) --token $(DATABRICKS_TOKEN) --cluster $(EXISTING_CLUSTER_ID) --localpath $(Build.Repository.LocalPath)/pipeline/ML/deploy --workspacepath /Demo/Test --outfilepath /home/vsts/work/1/s/pipeline --params model_name=$(MODEL_NAME),stage="production,phase="prod"'
#  displayName: 'Deploy mlFlow Model from Registry to Azure ML into Production'