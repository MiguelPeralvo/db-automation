# mlFlow Registry QA & Prod pipeline

variables:
- group: Databricks-environment
- group: Pipeline-variables

trigger:
  branches:
    include:
      - dev
      - master
      - staging
      - releases/*

stages:
- stage: Build
  jobs:
  - job: Build
    pool:
      vmImage: 'ubuntu-18.04'

    steps:
    - task: UsePythonVersion@0
      displayName: 'Use Python 3.7'
      inputs:
        versionSpec: 3.7

    - script: |
        pip install pytest requests setuptools wheel pyspark==2.4.5
    #    pip install -U databricks-cli
      displayName: 'Load Python Dependencies'

    - checkout: self
      persistCredentials: true
      clean: true
      displayName: 'Checkout & Build.Reason: $(Build.Reason) & Build.SourceBranchName: $(Build.SourceBranchName)'

    #- script: |
    #    git status
    #    git checkout
    ##   git checkout $(Build.SourceBranch)
    #  displayName: 'Get Latest from Branch $(Build.SourceBranchName) / $(Build.SourceBranch) / $(branchName)'

    - script: |
        python -m pytest --junit-xml=$(Build.Repository.LocalPath)/logs/TEST-LOCAL.xml $(Build.Repository.LocalPath)/libraries/python/dbxdemo/test*.py || true
        ls logs
      displayName: 'Run Python Unit Tests for library code'

    - task: PublishTestResults@2
      inputs:
        testResultsFiles: '**/TEST-*.xml'
        failTaskOnFailedTests: true
        publishRunAttachments: true

    - script: |
        cd $(Build.Repository.LocalPath)/libraries/python/dbxdemo
        python3 setup.py sdist bdist_wheel
        ls dist/
      displayName: 'Build Python Wheel for Libs'

    - task: riserrad.azdo-databricks.azdo-databricks-configuredatabricks.configuredatabricks@0
      inputs:
        url: '$(WORKSPACE_REGION_URL)/?o=$(WORKSPACE_ORG_ID)'
        token: '$(DATABRICKS_TOKEN)'
      displayName: 'Configure Databricks CLI for AZDO'

      #- script: |
      #    dbfs ls
      #  displayName: 'Check config working'

    - script: |
        cat /home/vsts/.databrickscfg
        echo ""
        echo "-------------"
        databricks workspace mkdirs /Shared/db-automation/train --profile AZDO
        databricks workspace import $(Build.Repository.LocalPath)/"pipeline/ML/train/train_model.py" "/Shared/db-automation/train/train_model"  --language PYTHON -o --profile AZDO
      displayName: 'Import ML Train Notebook'

    - task: PythonScript@0
      inputs:
        scriptSource: 'filePath'
        scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/executenotebook.py'
        arguments: '--shard $(WORKSPACE_REGION_URL) --token $(DATABRICKS_TOKEN) --cluster $(EXISTING_CLUSTER_ID) --localpath $(Build.Repository.LocalPath)/pipeline/ML/train --workspacepath /Shared/db-automation/train --outfilepath /home/vsts/work/1/s/pipeline --params model_name=$(MODEL_NAME)'
      displayName: 'Train the new version of the model'

    - script: |
        cat /home/vsts/.databrickscfg
        echo ""
        echo "-------------"
        databricks workspace mkdirs /Shared/db-automation/batch_test --profile AZDO
        databricks workspace import $(Build.Repository.LocalPath)/"pipeline/ML/batch_test/deploy_test_databricks_batch_ml_model.py" "/Shared/db-automation/batch_test/deploy_test_databricks_batch_ml_model"  --language PYTHON -o --profile AZDO
      displayName: 'Import ML Batch Deploy/Test Notebook'

    - task: PythonScript@0
      inputs:
        scriptSource: 'filePath'
        scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/executenotebook.py'
        arguments: '--shard $(WORKSPACE_REGION_URL) --token $(DATABRICKS_TOKEN) --cluster $(EXISTING_CLUSTER_ID) --localpath $(Build.Repository.LocalPath)/pipeline/ML/batch_test --workspacepath /Shared/db-automation/batch_test --outfilepath /home/vsts/work/1/s/pipeline --params model_name=$(MODEL_NAME)'
      displayName: 'Deploy and test mlFlow Model from Registry to Databricks batch'

    - script: |
        echo $(response)
      displayName: 'Batch test result'

- stage: Release
  dependsOn: Build
  condition: and(succeeded(), startsWith(variables['Build.SourceBranchName'], 'releases'))
  jobs:
  - job: Release
    pool:
      vmImage: 'ubuntu-18.04'

    steps:
    - task: UsePythonVersion@0
      displayName: 'Use Python 3.7'
      inputs:
        versionSpec: 3.7

    - script: |
        echo "Release"
      displayName: 'Release stage'

#- script: |
#    cat /home/vsts/.databrickscfg
#    echo ""
#    echo "-------------"
#    databricks workspace mkdirs /Shared/db-automation/deploy --profile AZDO
#    databricks workspace import $(Build.Repository.LocalPath)/"pipeline/ML/deploy/deploy_azure_ml_model.py" "/Shared/db-automation/deploy/deploy_azure_ml_model"  --language PYTHON -o --profile AZDO
#  displayName: 'Import ML Deploy Notebook'
#
#- task: PythonScript@0
#  inputs:
#    scriptSource: 'filePath'
#    scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/executenotebook.py'
#    arguments: '--shard $(WORKSPACE_REGION_URL) --token $(DATABRICKS_TOKEN) --cluster $(EXISTING_CLUSTER_ID) --localpath $(Build.Repository.LocalPath)/pipeline/ML/deploy --workspacepath /Shared/db-automation/deploy --outfilepath /home/vsts/work/1/s/pipeline --params model_name=$(MODEL_NAME)'
#  displayName: 'Deploy mlFlow Model from Registry to Azure ML for Testing'
#
#- script: |
#    echo $(response)
#  displayName: 'API URL'
#
#- script: |
#    cat /home/vsts/.databrickscfg
#    echo ""
#    echo "-------------"
#    databricks workspace mkdirs /Shared/db-automation/test --profile AZDO
#    databricks workspace import $BUILD_SOURCESDIRECTORY/"pipeline/ML/test/test_api.py" "/Shared/db-automation/test/test_api"  --language PYTHON -o --profile AZDO
#  displayName: 'Import ML Test Notebook'
#
#- task: PythonScript@0
#  inputs:
#    scriptSource: 'filePath'
#    scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/executenotebook.py'
#    arguments: '--shard $(WORKSPACE_REGION_URL) --token $(DATABRICKS_TOKEN) --cluster $(EXISTING_CLUSTER_ID) --localpath $(Build.Repository.LocalPath)/pipeline/ML/test --workspacepath /Shared/db-automation/test --outfilepath /home/vsts/work/1/s/pipeline --params model_name=$(MODEL_NAME),scoring_uri=$(response)'
#  displayName: 'Test mlFlow Model from Registry against REST API'

#- task: PythonScript@0
#  inputs:
#    scriptSource: 'filePath'
#    scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/mlflow.py'
#    arguments: ''
#  displayName: 'Promote mlFlow Registry model to Production'
#
#- script: |
#    echo $(response)
#  displayName: 'Model Production Version'
#
#- task: PythonScript@0
#  inputs:
#    scriptSource: 'filePath'
#    scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/executenotebook.py'
#    arguments: '--shard $(WORKSPACE_REGION_URL) --token $(DATABRICKS_TOKEN) --cluster $(EXISTING_CLUSTER_ID) --localpath $(Build.Repository.LocalPath)/pipeline/ML/deploy --workspacepath /Demo/Test --outfilepath /home/vsts/work/1/s/pipeline --params model_name=$(MODEL_NAME),stage="production,phase="prod"'
#  displayName: 'Deploy mlFlow Model from Registry to Azure ML into Production'