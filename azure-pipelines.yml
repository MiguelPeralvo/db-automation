# mlFlow Registry QA & Prod pipeline

variables:
- group: Databricks-environment
- group: Pipeline-variables
- group: Service-Principal-Variables

schedules:
- cron: "00 00 * * *"
  displayName: Daily midnight build
  branches:
    include:
      - master

trigger:
  branches:
    include:
      - dev
      - master
      - release
      - staging
      - releases/*
  tags:
    include:
      - v*.*
      - prod

  paths:
    include:
      - libraries/*
      - pipeline/*
      - cicd-scripts/*
      - azure-pipelines.yml

stages:
- stage: Info
  condition: True
  jobs:
  - job: Info
    pool:
      vmImage: 'ubuntu-18.04'

    steps:
    - script: env | sort
      displayName: 'Environment / Context'

- stage: Build
  displayName: "CI/Build"
  condition: |
    and
    (
      ne(variables['Build.SourceBranch'], 'refs/heads/adf_publish'),
      ne(variables['Build.SourceBranch'], 'refs/heads/master'),
      ne(variables['Build.SourceBranch'], 'refs/heads/release'),
      not(startsWith(variables['Build.SourceBranch'], 'refs/heads/releases')),
      not(startsWith(variables['Build.SourceBranch'], 'refs/tags/v')),
      ne(variables['Build.SourceBranch'], 'refs/tags/production')
    )
  jobs:
  - job: Build
    pool:
      vmImage: 'ubuntu-18.04'

    steps:
    - script: env | sort
      displayName: 'Environment / Context'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.7'
      inputs:
        versionSpec: 3.7

    - script: |
        pip install pytest requests setuptools wheel pyspark==2.4.5
    #    pip install -U databricks-cli
      displayName: 'Load Python Dependencies'

    - checkout: self
      persistCredentials: true
      clean: true
      displayName: 'Checkout & Build.Reason: $(Build.Reason) & Build.SourceBranchName: $(Build.SourceBranchName)'

    - script: |
        echo "Build.SourceBranchName: $(Build.SourceBranchName)"
        if [ $(Build.SourceBranchName) != "merge" ]
        then
          git checkout $(Build.SourceBranchName)
        fi

      displayName: 'Get Latest from Branch $(Build.SourceBranchName) if not in PR'

    - script: |
        python -m pytest --junit-xml=$(Build.Repository.LocalPath)/logs/TEST-LOCAL.xml $(Build.Repository.LocalPath)/libraries/python/dbxdemo/test*.py || true
        ls logs
      displayName: 'Run Python Unit Tests for library code'

    - task: PublishTestResults@2
      inputs:
        testResultsFiles: '**/TEST-*.xml'
        failTaskOnFailedTests: true
        publishRunAttachments: true

    - task: riserrad.azdo-databricks.azdo-databricks-configuredatabricks.configuredatabricks@0
      inputs:
        url: '$(WORKSPACE_REGION_URL)/?o=$(WORKSPACE_ORG_ID)'
        token: '$(DATABRICKS_TOKEN)'
      displayName: 'Configure Databricks CLI for AZDO'

    - script: |
        cd $(Build.Repository.LocalPath)/libraries/python/dbxdemo
        python3 setup.py sdist bdist_wheel
        ls dist/
        export WHEEL_PATH=`ls $(Build.Repository.LocalPath)/libraries/python/dbxdemo/dist/*.whl`
        export WHEEL_FILENAME=`basename $WHEEL_PATH`
        dbfs rm --recursive dbfs:/FileStore/Shared/db-automation/libraries/$WHEEL_FILENAME --profile AZDO
        dbfs cp --overwrite $WHEEL_PATH dbfs:/FileStore/Shared/db-automation/libraries/$WHEEL_FILENAME --profile AZDO
        mkdir -p $(Build.BinariesDirectory)/drop/
        export BINARY_WHEEL_PATH=$(Build.BinariesDirectory)/drop/$WHEEL_FILENAME
        cp $WHEEL_PATH $BINARY_WHEEL_PATH
        echo "##vso[task.setvariable variable=WHEEL_PATH_GLOBAL]$BINARY_WHEEL_PATH"
      displayName: 'Build Python Wheel for Libs and upload'

    - task: PublishBuildArtifacts@1
      inputs:
          pathtoPublish: $(WHEEL_PATH_GLOBAL)
          artifactName: drop
      displayName: 'Publish Artifact: drop'

    - script: |
        if [ $(Build.SourceBranchName) != "release" ] && [ $(Build.Reason) = "IndividualCI" ]
        then
          cat /home/vsts/.databrickscfg
          echo ""
          echo "-------------"
          dbfs cp --overwrite $(Build.Repository.LocalPath)/"pipeline/ML/train/train_model.py" dbfs:/FileStore/Shared/db-automation/train/train_model.py --profile AZDO
          python $(Build.Repository.LocalPath)/cicd-scripts/execute_script.py  --shard $(WORKSPACE_REGION_URL) --token $(DATABRICKS_TOKEN) --cluster $(EXISTING_CLUSTER_ID) --dbfspath dbfs:/FileStore/Shared/db-automation/train/train_model.py --outfilepath /home/vsts/work/1/s/pipeline --params "\-e,/Shared/db-automation/train/train_model,\-m,$(MODEL_NAME),\-r,dbfs:/FileStore/Shared/db-automation"
        else
          echo "We're not training/registering the model in this event"
        fi
      displayName: 'Train and register the best version of the model. Build.SourceBranchName: $(Build.SourceBranchName). Build.Reason:  $(Build.Reason)'

    - script: |
        if [ $(Build.SourceBranchName) != "release" ] && [ $(Build.Reason) = "IndividualCI" ]
        then
          cat /home/vsts/.databrickscfg
          echo ""
          echo "-------------"
          dbfs cp --overwrite $(Build.Repository.LocalPath)/"pipeline/ML/batch_test/deploy_test_databricks_batch_ml_model.py" dbfs:/FileStore/Shared/db-automation/batch_test/deploy_test_databricks_batch_ml_model.py --profile AZDO
          python $(Build.Repository.LocalPath)/cicd-scripts/execute_script.py --shard $(WORKSPACE_REGION_URL) --token $(DATABRICKS_TOKEN) --cluster $(EXISTING_CLUSTER_ID) --dbfspath dbfs:/FileStore/Shared/db-automation/batch_test/deploy_test_databricks_batch_ml_model.py --outfilepath /home/vsts/work/1/s/pipeline --params "\-m,$(MODEL_NAME),\-r,dbfs:/FileStore/Shared/db-automation,\-s,staging"
        else
          echo "We're not testing the model in this event"
        fi
      displayName: 'Deploy and test mlFlow Model from Registry to Databricks batch. Build.SourceBranchName: $(Build.SourceBranchName). Build.Reason:  $(Build.Reason)'

- stage: Release
  displayName: "CD/Release"
#  dependsOn: [Build]
  condition: |
    or(
      and(
        eq(variables['Build.Reason'], 'PullRequest'),
        startsWith(variables['Build.SourceBranch'], 'refs/pull'),
        eq(variables['System.PullRequest.TargetBranch'], 'release')
      ),
      eq(variables['Build.SourceBranch'], 'refs/heads/release'),
      startsWith(variables['Build.SourceBranch'], 'refs/heads/releases'),
      startsWith(variables['Build.SourceBranch'], 'refs/tags/v'),
      eq(variables['Build.SourceBranch'], 'refs/tags/production')
    )
  jobs:
  - deployment: Staging
    condition: |
      and(
        eq(variables['Build.Reason'], 'PullRequest'),
        startsWith(variables['Build.SourceBranch'], 'refs/pull'),
        eq(variables['System.PullRequest.TargetBranch'], 'release')
      )
    displayName: Deployment in Staging for integration testing
    pool:
      vmImage: 'ubuntu-18.04'
    environment: Staging
    strategy:
      runOnce:
        deploy:
          steps:
          - script: env | sort
            displayName: 'Environment / Context'

#          - task: DownloadBuildArtifacts@0
#            displayName: 'Download Build Artifacts'
#            inputs:
#              buildType: specific
#              project: '$(System.TeamProjectId)'
#              pipeline: 1
#              buildVersionToDownload: latestFromBranch
#              branchName: '$(System.PullRequest.SourceBranch)'
#              artifactName: drop

          - task: UsePythonVersion@0
            displayName: 'Use Python 3.7'
            inputs:
              versionSpec: 3.7

          - script: |
              echo "Staging"
            displayName: 'Staging stage'

          - checkout: self
            persistCredentials: true
            clean: true
            displayName: 'Checkout & Build.Reason: $(Build.Reason) & Build.SourceBranchName: $(Build.SourceBranchName)'

          - script: |
              echo "Build.SourceBranchName: $(Build.SourceBranchName)"
              if [ $(Build.SourceBranchName) != "merge" ]
              then
                git checkout $(Build.SourceBranchName)
              fi

            displayName: 'Get Latest from Branch $(Build.SourceBranchName) if not in PR'

          - script: |
              pip install azure-cli-core azure-mgmt-resource azure-mgmt-datafactory requests mlflow==1.8.0
          #    pip install -U databricks-cli
            displayName: 'Load Python Dependencies'

          - task: riserrad.azdo-databricks.azdo-databricks-configuredatabricks.configuredatabricks@0
            inputs:
              url: '$(STAGING_WORKSPACE_REGION_URL)/?o=$(STAGING_WORKSPACE_ORG_ID)'
              token: '$(STAGING_DATABRICKS_TOKEN)'
            displayName: 'Configure Databricks CLI for AZDO for STAGING'


          - script: |
              cat /home/vsts/.databrickscfg
              echo ""
              echo "-------------"
              echo $(Build.Repository.LocalPath)
              ls -latr $(Build.Repository.LocalPath)/"pipeline/ML/inference"
              dbfs mkdirs dbfs:/FileStore/Shared/db-automation/ML  --profile AZDO
              dbfs cp --overwrite $(Build.Repository.LocalPath)/"pipeline/ETL/lib_use/files.py" dbfs:/FileStore/Shared/db-automation/ETL/files.py --profile AZDO
              dbfs cp --overwrite $(Build.Repository.LocalPath)/"pipeline/ML/feature_engineering/fe_pre_model.py" dbfs:/FileStore/Shared/db-automation/ML/fe_pre_model.py --profile AZDO
              dbfs cp --overwrite $(Build.Repository.LocalPath)/"pipeline/ML/inference/batch_model.py" dbfs:/FileStore/Shared/db-automation/ML/batch_model.py --profile AZDO
              dbfs ls dbfs:/FileStore/Shared/db-automation  --profile AZDO
            displayName: 'Import ML Batch Inference script'

          - script: |
              id
              echo $HOME
              echo "[registry]" > $HOME/.databrickscfg
              echo "host=$(WORKSPACE_REGION_URL)" >> $HOME/.databrickscfg
              echo "token=$(DATABRICKS_TOKEN)" >> $HOME/.databrickscfg
              cat /home/vsts/.databrickscfg
              mkdir -p /home/vsts/mlflow/$(MODEL_NAME)/artifacts
            displayName: 'Configure mlflow client connection to mlflow global (per org) model registry'

          - task: PythonScript@0
            inputs:
              scriptSource: 'filePath'
              scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/remote_registry_mlflow.py'
              arguments: '--output_local_path=/home/vsts/mlflow/$(MODEL_NAME)/artifacts --model $(MODEL_NAME)'
            displayName: 'Retrieve artifacts from the mlflow global (per org) model registry to use them in Databricks Staging'

          - task: liprec.vsts-publish-adf.deploy-adf-json.deploy-adf-json@2
            displayName: 'Deploy Data Pipeline to $(STAGING_ADF_NAME) ADF'
            inputs:
              azureSubscription: '$(AZURE_SUBSCRIPTION_ID)'
              ResourceGroupName: '$(RESOURCE_GROUP)'
              DatafactoryName: '$(STAGING_ADF_NAME)'
              PipelinePath: $(Build.Repository.LocalPath)/resources/adf/pipeline/data_ml_pipeline.json

          - task: AzureCLI@2
            displayName: 'Trigger Azure Data Factory Pipeline $(STAGING_ADF_NAME)/$(STAGING_ADF_PIPELINE_NAME)'
            inputs:
              azureSubscription: '$(AZURE_SUBSCRIPTION_ID)'
              scriptType: bash
              scriptLocation: inlineScript
              inlineScript: |
                python $(Build.Repository.LocalPath)/cicd-scripts/adf_pipeline_run.py -r $(RESOURCE_GROUP) -a $(STAGING_ADF_NAME) -p $(STAGING_ADF_PIPELINE_NAME) -o ./logs/json -pa "{\"environment\":\"-s staging\"}"
              useGlobalConfig: true
              timeoutInMinutes: 10

  - deployment: Production
    condition: |
      or(
        eq(variables['Build.SourceBranch'], 'refs/heads/release'),
        startsWith(variables['Build.SourceBranch'], 'refs/heads/releases'),
        startsWith(variables['Build.SourceBranch'], 'refs/tags/v'),
        eq(variables['Build.SourceBranch'], 'refs/tags/production')
      )
    displayName: Deployment of release in production
    pool:
      vmImage: 'ubuntu-18.04'
    environment: production
    strategy:
      runOnce:
        deploy:
          steps:
          - script: env | sort
            displayName: 'Environment / Context'

          - task: UsePythonVersion@0
            displayName: 'Use Python 3.7'
            inputs:
              versionSpec: 3.7

          - script: |
              echo "production"
            displayName: 'production stage'

          - checkout: self
            persistCredentials: true
            clean: true
            displayName: 'Checkout & Build.Reason: $(Build.Reason) & Build.SourceBranchName: $(Build.SourceBranchName)'

          - script: |
              echo "Build.SourceBranchName: $(Build.SourceBranchName)"
              if [ $(Build.SourceBranchName) != "merge" ]
              then
                git checkout $(Build.SourceBranchName)
              fi

            displayName: 'Get Latest from Branch $(Build.SourceBranchName) if not in PR'

          - script: |
              pip install azure-cli-core azure-mgmt-resource azure-mgmt-datafactory requests mlflow==1.8.0
          #    pip install -U databricks-cli
            displayName: 'Load Python Dependencies'

          - task: riserrad.azdo-databricks.azdo-databricks-configuredatabricks.configuredatabricks@0
            inputs:
              url: '$(PRODUCTION_WORKSPACE_REGION_URL)/?o=$(PRODUCTION_WORKSPACE_ORG_ID)'
              token: '$(PRODUCTION_DATABRICKS_TOKEN)'
            displayName: 'Configure Databricks CLI for AZDO for PRODUCTION'


          - script: |
              cat /home/vsts/.databrickscfg
              echo ""
              echo "-------------"
              echo $(Build.Repository.LocalPath)
              ls -latr $(Build.Repository.LocalPath)/"pipeline/ML/inference"
              dbfs mkdirs dbfs:/FileStore/Shared/db-automation/ML  --profile AZDO
              dbfs cp --overwrite $(Build.Repository.LocalPath)/"pipeline/ETL/lib_use/files.py" dbfs:/FileStore/Shared/db-automation/ETL/files.py --profile AZDO
              dbfs cp --overwrite $(Build.Repository.LocalPath)/"pipeline/ML/feature_engineering/fe_pre_model.py" dbfs:/FileStore/Shared/db-automation/ML/fe_pre_model.py --profile AZDO
              dbfs cp --overwrite $(Build.Repository.LocalPath)/"pipeline/ML/inference/batch_model.py" dbfs:/FileStore/Shared/db-automation/ML/batch_model.py --profile AZDO
              dbfs ls dbfs:/FileStore/Shared  --profile AZDO
            displayName: 'Import ML Batch Inference script'

          - script: |
              id
              echo $HOME
              echo "[registry]" > $HOME/.databrickscfg
              echo "host=$(WORKSPACE_REGION_URL)" >> $HOME/.databrickscfg
              echo "token=$(DATABRICKS_TOKEN)" >> $HOME/.databrickscfg
              cat /home/vsts/.databrickscfg
              mkdir -p /home/vsts/mlflow/$(MODEL_NAME)/artifacts
            displayName: 'Configure mlflow client connection to global mlflow global (per org) model registry'

          - task: PythonScript@0
            inputs:
              scriptSource: 'filePath'
              scriptPath: '$(Build.Repository.LocalPath)/cicd-scripts/remote_registry_mlflow.py'
              arguments: '--output_local_path=/home/vsts/mlflow/$(MODEL_NAME)/artifacts --model $(MODEL_NAME)'
            displayName: 'Retrieve artifacts from the mlflow global (per org) model registry to use them in Databricks Production'

          - task: liprec.vsts-publish-adf.deploy-adf-json.deploy-adf-json@2
            displayName: 'Deploy Data Pipeline to $(PRODUCTION_ADF_NAME) ADF'
            inputs:
              azureSubscription: '$(AZURE_SUBSCRIPTION_ID)'
              ResourceGroupName: '$(RESOURCE_GROUP)'
              DatafactoryName: '$(PRODUCTION_ADF_NAME)'
              PipelinePath: $(Build.Repository.LocalPath)/resources/adf/pipeline/data_ml_pipeline.json